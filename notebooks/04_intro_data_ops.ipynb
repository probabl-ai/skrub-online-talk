{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ff90cb",
   "metadata": {},
   "source": [
    "\n",
    "# Machine learning pipelines with multiple tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skrub\n",
    "\n",
    "dataset = skrub.datasets.fetch_credit_fraud(split=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "skrub.TableReport(dataset.baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "skrub.TableReport(dataset.products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34cf00",
   "metadata": {},
   "source": [
    "\n",
    "## The current state\n",
    "\n",
    "Let's ask an LLM to create a machine learning pipeline for the given dataset that is\n",
    "composed of two tables.\n",
    "\n",
    "**ðŸ¤– Prompt:**\n",
    "\n",
    "*I would like you to write a machine learning pipeline.\n",
    "\n",
    "The dataset is composed of two tables: `dataset.baskets` and `data.product`. The\n",
    "variable to predict is the column \"fraud_flag\" from the `data.baskets` table.\n",
    "\n",
    "So to create the a machine learning pipeline, you will need to kind of join the\n",
    "information between the two table and train and test a machine learning model for it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tables to create features\n",
    "# First, let's aggregate product information by basket\n",
    "basket_features = (\n",
    "    dataset.products.groupby(\"basket_ID\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"cash_price\": [\"sum\", \"mean\", \"std\", \"min\", \"max\", \"count\"],\n",
    "            \"Nbr_of_prod_purchas\": [\"sum\", \"mean\", \"std\"],\n",
    "            \"item\": \"nunique\",  # Number of unique items\n",
    "            \"make\": \"nunique\",  # Number of unique makes\n",
    "            \"model\": \"nunique\",  # Number of unique models\n",
    "            \"goods_code\": \"nunique\",  # Number of unique goods codes\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "basket_features.columns = [\"basket_ID\"] + [\n",
    "    f\"{col[0]}_{col[1]}\" for col in basket_features.columns[1:]\n",
    "]\n",
    "\n",
    "# Add additional features\n",
    "basket_features[\"avg_price_per_item\"] = (\n",
    "    basket_features[\"cash_price_sum\"] / basket_features[\"Nbr_of_prod_purchas_sum\"]\n",
    ")\n",
    "basket_features[\"price_std_norm\"] = basket_features[\"cash_price_std\"] / (\n",
    "    basket_features[\"cash_price_mean\"] + 1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skrub.TableReport(basket_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912944d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with fraud labels\n",
    "df = dataset.baskets.merge(\n",
    "    basket_features, right_on=\"basket_ID\", left_on=\"ID\", how=\"left\"\n",
    ")\n",
    "skrub.TableReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features for fraud detection\n",
    "# These are domain-specific features that might be indicative of fraud\n",
    "\n",
    "# 1. Price anomaly features\n",
    "df[\"price_anomaly\"] = (\n",
    "    df[\"cash_price_sum\"] > df[\"cash_price_sum\"].quantile(0.95)\n",
    ").astype(int)\n",
    "df[\"low_price_anomaly\"] = (\n",
    "    df[\"cash_price_sum\"] < df[\"cash_price_sum\"].quantile(0.05)\n",
    ").astype(int)\n",
    "\n",
    "# 2. Quantity anomaly features\n",
    "df[\"quantity_anomaly\"] = (\n",
    "    df[\"Nbr_of_prod_purchas_sum\"] > df[\"Nbr_of_prod_purchas_sum\"].quantile(0.95)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Diversity features\n",
    "df[\"item_diversity\"] = df[\"item_nunique\"] / (df[\"Nbr_of_prod_purchas_sum\"] + 1e-8)\n",
    "df[\"make_diversity\"] = df[\"make_nunique\"] / (df[\"Nbr_of_prod_purchas_sum\"] + 1e-8)\n",
    "\n",
    "# 4. Price consistency features\n",
    "df[\"price_consistency\"] = 1 / (\n",
    "    df[\"cash_price_std\"] + 1e-8\n",
    ")  # Higher values = more consistent prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skrub.TableReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to be aligned with the next example\n",
    "id_split = 76543  # noqa\n",
    "df_train = df.query(\"ID <= @id_split\")\n",
    "df_test = df.query(\"ID > @id_split\")\n",
    "\n",
    "X_train = df_train.drop([\"basket_ID\", \"ID\", \"fraud_flag\"], axis=1)\n",
    "y_train = df_train[\"fraud_flag\"]\n",
    "X_test = df_test.drop([\"basket_ID\", \"ID\", \"fraud_flag\"], axis=1)\n",
    "y_test = df_test[\"fraud_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdcc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "X_train = X_train.fillna(0)  # Fill NaN with 0 for numerical features\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892bcac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline using skrub's TableVectorizer\n",
    "preprocessor = skrub.TableVectorizer(\n",
    "    drop_if_constant=True,\n",
    "    high_cardinality=skrub.TextEncoder(),\n",
    "    low_cardinality=skrub.StringEncoder(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120422c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "# Train multiple models and compare performance\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"Gradient Boosting\": HistGradientBoostingClassifier(random_state=42, max_iter=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    results[name] = {\n",
    "        \"pipeline\": pipeline,\n",
    "        \"predictions\": y_pred,\n",
    "        \"probabilities\": y_pred_proba,\n",
    "        \"auc_score\": auc_score,\n",
    "    }\n",
    "\n",
    "    print(f\"{name} - AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8eec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the best model (Random Forest typically performs well on tabular data)\n",
    "best_model_name = \"Random Forest\"\n",
    "best_pipeline = results[best_model_name][\"pipeline\"]\n",
    "y_pred = results[best_model_name][\"predictions\"]\n",
    "y_pred_proba = results[best_model_name][\"probabilities\"]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"AUC Score: {results[best_model_name]['auc_score']:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb813f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(\"Fraud Detection Model Evaluation\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Confusion Matrix\")\n",
    "axes[0, 0].set_xlabel(\"Predicted\")\n",
    "axes[0, 0].set_ylabel(\"Actual\")\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0, 1].plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    linewidth=2,\n",
    "    label=f'ROC Curve (AUC = {results[best_model_name][\"auc_score\"]:.3f})',\n",
    ")\n",
    "axes[0, 1].plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "axes[0, 1].set_xlabel(\"False Positive Rate\")\n",
    "axes[0, 1].set_ylabel(\"True Positive Rate\")\n",
    "axes[0, 1].set_title(\"ROC Curve\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature Importance (for Random Forest)\n",
    "if hasattr(best_pipeline.named_steps[\"classifier\"], \"feature_importances_\"):\n",
    "    feature_names = best_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    importances = best_pipeline.named_steps[\"classifier\"].feature_importances_\n",
    "\n",
    "    # Get top 10 most important features\n",
    "    top_indices = np.argsort(importances)[-10:]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importances = importances[top_indices]\n",
    "\n",
    "    axes[1, 0].barh(range(len(top_features)), top_importances)\n",
    "    axes[1, 0].set_yticks(range(len(top_features)))\n",
    "    axes[1, 0].set_yticklabels(top_features)\n",
    "    axes[1, 0].set_xlabel(\"Feature Importance\")\n",
    "    axes[1, 0].set_title(\"Top 10 Most Important Features\")\n",
    "\n",
    "# 4. Model Comparison\n",
    "model_names = list(results.keys())\n",
    "auc_scores = [results[name][\"auc_score\"] for name in model_names]\n",
    "axes[1, 1].bar(model_names, auc_scores, color=[\"skyblue\", \"lightgreen\", \"lightcoral\"])\n",
    "axes[1, 1].set_ylabel(\"AUC Score\")\n",
    "axes[1, 1].set_title(\"Model Comparison\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "for i, v in enumerate(auc_scores):\n",
    "    axes[1, 1].text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef490b",
   "metadata": {},
   "source": [
    "\n",
    "**Pitfalls:**\n",
    "- Data leakage\n",
    "- Headache to replay the same code for a new batch of fresh data\n",
    "- Hard to tune hyperparameters related to the preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b977f5f",
   "metadata": {},
   "source": [
    "\n",
    "## The `skrub` Data Operations (DataOps)\n",
    "\n",
    "DataOps extend the scikit-learn machinery to allow more complex data operations but\n",
    "taking care about machine learning states (i.e. fit / predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = skrub.datasets.fetch_credit_fraud(split=\"train\")\n",
    "baskets_experiment, products_experiment = dataset.baskets, dataset.products\n",
    "dataset = skrub.datasets.fetch_credit_fraud(split=\"test\")\n",
    "baskets_production, products_production = dataset.baskets, dataset.products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca4265",
   "metadata": {},
   "source": [
    "\n",
    "To achieve the traceability of the data operations, `skrub` records them. We call\n",
    "this a DataOps plan and it is a graph of the data operations. We can record several\n",
    "type of operations:\n",
    "\n",
    "- any dataframe operation (e.g. merge, group by, aggregate, etc.)\n",
    "- scikit-learn estimators (e.g. `StandardScaler`, `LogisticRegression`, etc.)\n",
    "- any custom user code (e.g. load data from path, any data transformation, etc.)\n",
    "\n",
    "Everything starts from a `skrub` variable that is a symbolic representation of the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = skrub.var(\"products\")\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459f96d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "There is not much for the moment. But what if we would like to apply a transformation.\n",
    "We can take the previous transform that the LLM created to aggregate statistics for\n",
    "a given basket.\n",
    "\n",
    "The trick here is that we don't want to evaluate the function directly but record\n",
    "that we should apply it. Therefore, we use the `@skrub.deferred` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def aggregate_basket_features(products):\n",
    "    basket_features = (\n",
    "        products.groupby(\"basket_ID\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"cash_price\": [\"sum\", \"mean\", \"std\", \"min\", \"max\", \"count\"],\n",
    "                \"Nbr_of_prod_purchas\": [\"sum\", \"mean\", \"std\"],\n",
    "                \"item\": \"nunique\",  # Number of unique items\n",
    "                \"make\": \"nunique\",  # Number of unique makes\n",
    "                \"model\": \"nunique\",  # Number of unique models\n",
    "                \"goods_code\": \"nunique\",  # Number of unique goods codes\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Flatten column names\n",
    "    basket_features.columns = [\"basket_ID\"] + [\n",
    "        f\"{col[0]}_{col[1]}\" for col in basket_features.columns[1:]\n",
    "    ]\n",
    "\n",
    "    # Add additional features\n",
    "    basket_features[\"avg_price_per_item\"] = (\n",
    "        basket_features[\"cash_price_sum\"] / basket_features[\"Nbr_of_prod_purchas_sum\"]\n",
    "    )\n",
    "    basket_features[\"price_std_norm\"] = basket_features[\"cash_price_std\"] / (\n",
    "        basket_features[\"cash_price_mean\"] + 1e-8\n",
    "    )\n",
    "\n",
    "    return basket_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_features = aggregate_basket_features(products)\n",
    "basket_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62930a8e",
   "metadata": {},
   "source": [
    "\n",
    "Applying the function do not execute anything but is adding a new node in the DataOps\n",
    "plan. However, you can imagine that building a complex DataOps plan without having\n",
    "eager feedback would not be practical.\n",
    "\n",
    "It is one reason, that you can attach some data to the `skrub` variable to compute\n",
    "a preview on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7555a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = skrub.var(\"products\", products_experiment)\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_features = aggregate_basket_features(products)\n",
    "basket_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66936cd1",
   "metadata": {},
   "source": [
    "\n",
    "Now, you have both the graph and the preview of the transformation on your data.\n",
    "\n",
    "We can do the same on the baskets table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c06d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = skrub.var(\"baskets\", baskets_experiment)\n",
    "baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970007e",
   "metadata": {},
   "source": [
    "\n",
    "However, here we see that the preview is done on the full training set. We might be\n",
    "interested in just having the preview on a subsample of the data at least for the\n",
    "debugging phase when building our DataOps plan. It is the job of the `skb.subsample`\n",
    "method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = baskets.skb.subsample(n=5_000)\n",
    "baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640c128",
   "metadata": {},
   "source": [
    "\n",
    "When we previously discussed pitfalls of the LLM generated code, we mentioned issue\n",
    "related to data leakage. Usually, this problem boils down to not compute statistics\n",
    "on the training set and apply them on the test set and instead use the full dataset.\n",
    "\n",
    "Therefore, we introduce the `skb.mark_as_X` and `skb.mark_as_y` methods to mark nodes\n",
    "in the DataOps plan as the features and target variables. It informs that any\n",
    "subsequent machine learning operations relying on training / testing sets should\n",
    "start from the marked nodes. In short, if a split should happen, it should be done\n",
    "at the marked nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e580e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = baskets[[\"ID\"]].skb.mark_as_X()\n",
    "target = baskets[\"fraud_flag\"].skb.mark_as_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13919b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd9ef4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Now, we are going to apply the different operations that the LLM came with. First, we\n",
    "need to join the baskets and the aggregated products features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da38ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def join_basket_aggregated_products(baskets, basket_features):\n",
    "    return baskets.merge(\n",
    "        basket_features, right_on=\"basket_ID\", left_on=\"ID\", how=\"left\"\n",
    "    ).drop(columns=[\"basket_ID\", \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_features = join_basket_aggregated_products(features, basket_features)\n",
    "aggregated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c92f3",
   "metadata": {},
   "source": [
    "\n",
    "Then, some domain-specific features have been derived from the aggregated features.\n",
    "The original code was the following:\n",
    "\n",
    "```python\n",
    "# 1. Price anomaly features\n",
    "df[\"price_anomaly\"] = (\n",
    "    df[\"cash_price_sum\"] > df[\"cash_price_sum\"].quantile(0.95)\n",
    ").astype(int)\n",
    "df[\"low_price_anomaly\"] = (\n",
    "    df[\"cash_price_sum\"] < df[\"cash_price_sum\"].quantile(0.05)\n",
    ").astype(int)\n",
    "# 2. Quantity anomaly features\n",
    "df[\"quantity_anomaly\"] = (\n",
    "    df[\"Nbr_of_prod_purchas_sum\"] > df[\"Nbr_of_prod_purchas_sum\"].quantile(0.95)\n",
    ").astype(int)\n",
    "```\n",
    "\n",
    "This code relies on the computation of quantiles. The way it was programmed presented\n",
    "a data leakage issue because the quantiles were computed on the full dataset.\n",
    "\n",
    "So here, we would not to create a scikit-learn transformer to compute the quantiles\n",
    "on the training set and apply the transformation in a specific `transform` method to\n",
    "be able to apply the transformation on the test set.\n",
    "\n",
    "Hopefully, the `KBinsDiscretizer` from scikit-learn would provide a very similar\n",
    "transformation and we only need to select the columns and apply this transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6800ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Bins whose width are too small\", category=UserWarning\n",
    ")\n",
    "\n",
    "cols_derived_anomaly = [\"cash_price_sum\", \"Nbr_of_prod_purchas_sum\"]\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=10,\n",
    "    encode=\"onehot-dense\",\n",
    "    strategy=\"quantile\",\n",
    "    quantile_method=\"averaged_inverted_cdf\",\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "aggregated_features_with_anomaly = aggregated_features.skb.apply(\n",
    "    skrub.ApplyToCols(discretizer, cols=cols_derived_anomaly, keep_original=True)\n",
    ")\n",
    "aggregated_features_with_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102f043",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Then, we can apply the stateless operations as a normal user custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def add_domain_specific_features(df):\n",
    "    # 3. Diversity features\n",
    "    df[\"item_diversity\"] = df[\"item_nunique\"] / (df[\"Nbr_of_prod_purchas_sum\"] + 1e-8)\n",
    "    df[\"make_diversity\"] = df[\"make_nunique\"] / (df[\"Nbr_of_prod_purchas_sum\"] + 1e-8)\n",
    "\n",
    "    # 4. Price consistency features\n",
    "    df[\"price_consistency\"] = 1 / (df[\"cash_price_std\"] + 1e-8)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c406335",
   "metadata": {},
   "source": [
    "\n",
    "And we need to concatenate the two tables to have the final features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49860dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = add_domain_specific_features(aggregated_features_with_anomaly)\n",
    "engineered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86793b5c",
   "metadata": {},
   "source": [
    "\n",
    "Then, we can apply the predictive model to the final features and we will observe a\n",
    "preview of a fit/predict on the subsample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_model = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", HistGradientBoostingClassifier(random_state=42, max_iter=100)),\n",
    "    ]\n",
    ")\n",
    "predictions = engineered_features.skb.apply(predictive_model, y=target)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.skb.full_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38a73c",
   "metadata": {},
   "source": [
    "\n",
    "Now the question is how can I extract the DataOps plan and potentially reuse it later.\n",
    "\n",
    "It is where we introduced the concept of learner which is the resulting predictive\n",
    "pipeline which will expose the usual `fit` / `predict` scikit-learn API with a twist.\n",
    "To get a fitted learner, one needs to call the `skb.make_learner` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = predictions.skb.make_learner(fitted=True)\n",
    "learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd38c6",
   "metadata": {},
   "source": [
    "\n",
    "This learner can be serialized and deserialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(learner, \"../output/learner.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d17414",
   "metadata": {},
   "outputs": [],
   "source": [
    "deserialized_learner = joblib.load(\"../output/learner.joblib\")\n",
    "deserialized_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b9208",
   "metadata": {},
   "source": [
    "\n",
    "Since `skrub` uses symbolic variables, the `fit` and `predict` methods are extended\n",
    "to accept a dictionary of dataframes or series. Let's compute the score on the\n",
    "training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb050a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = deserialized_learner.predict_proba(\n",
    "    {\"baskets\": baskets_experiment, \"products\": products_experiment}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(baskets_experiment[\"fraud_flag\"], y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cf4a8",
   "metadata": {},
   "source": [
    "\n",
    "And on the production set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d261677",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = deserialized_learner.predict_proba(\n",
    "    {\"baskets\": baskets_production, \"products\": products_production}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(baskets_production[\"fraud_flag\"], y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f45f1",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced usages of the DataOps\n",
    "\n",
    "Two usages that we did not present here yet are:\n",
    "\n",
    "- how to cross-validate a DataOps plan\n",
    "- how to tune hyperparameters of the DataOps plan\n",
    "\n",
    "The former is quite straightforward as a DataOps expose a `.skb.cross_validate`\n",
    "method. Restarting from the `predictions` variable, we can cross-validate the\n",
    "DataOps plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.skb.cross_validate(scoring=\"roc_auc\", cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e93f2",
   "metadata": {},
   "source": [
    "\n",
    "When it comes hyperparameters tuning, `skrub` introduces some functions that can be\n",
    "used where you want to tweak the hyperparameters. Those functions are:\n",
    "\n",
    "- `choose_from`: select from the given list of options\n",
    "- `choose_int`: select an integer within a range\n",
    "- `choose_float`: select a float within a range\n",
    "- `choose_bool`: select a bool\n",
    "- `optional`: chooses whether to execute the given operation\n",
    "\n",
    "Let's give an example where we will tune the estimators as the LLM suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {name: skrub.tabular_pipeline(model) for name, model in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6112dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = engineered_features.skb.apply(\n",
    "    skrub.choose_from(models, name=\"predictor\"), y=target\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcffad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = predictions.skb.make_grid_search(\n",
    "    fitted=True, scoring=\"roc_auc\", cv=2, n_jobs=-1\n",
    ")\n",
    "search.results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c2ce1",
   "metadata": {},
   "source": [
    "\n",
    "And what if we would like to perform a nested cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4431fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_search = skrub.cross_validate(\n",
    "    predictions.skb.make_grid_search(scoring=\"roc_auc\", cv=2),\n",
    "    environment={\"baskets\": baskets_experiment, \"products\": products_experiment},\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    return_learner=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04721ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for learner in nested_search[\"learner\"]:\n",
    "    print(learner.results_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146bc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
