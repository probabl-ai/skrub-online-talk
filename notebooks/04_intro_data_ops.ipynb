{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c091d7a3",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Pipeline for Fraud Detection\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for fraud detection\n",
    "using the credit fraud dataset from skrub. The dataset contains two tables:\n",
    "- `baskets`: Contains basket IDs and fraud flags (target variable)\n",
    "- `products`: Contains product information linked to baskets via `basket_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skrub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skrub import TableVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = skrub.datasets.fetch_credit_fraud()\n",
    "print(\"Dataset keys:\", dataset.keys())\n",
    "print(\"Baskets shape:\", dataset.baskets.shape)\n",
    "print(\"Products shape:\", dataset.products.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data structure\n",
    "print(\"Baskets columns:\", dataset.baskets.columns.tolist())\n",
    "print(\"Products columns:\", dataset.products.columns.tolist())\n",
    "print(\"\\nBaskets head:\")\n",
    "print(dataset.baskets.head())\n",
    "print(\"\\nProducts head:\")\n",
    "print(dataset.products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d132a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality and target distribution\n",
    "print(\"Fraud distribution:\")\n",
    "print(dataset.baskets['fraud_flag'].value_counts())\n",
    "print(f\"Fraud rate: {dataset.baskets['fraud_flag'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nMissing values in baskets:\")\n",
    "print(dataset.baskets.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in products:\")\n",
    "print(dataset.products.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00063e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tables to create features\n",
    "# First, let's aggregate product information by basket\n",
    "basket_features = dataset.products.groupby('basket_ID').agg({\n",
    "    'cash_price': ['sum', 'mean', 'std', 'min', 'max', 'count'],\n",
    "    'Nbr_of_prod_purchas': ['sum', 'mean', 'std'],\n",
    "    'item': 'nunique',  # Number of unique items\n",
    "    'make': 'nunique',  # Number of unique makes\n",
    "    'model': 'nunique',  # Number of unique models\n",
    "    'goods_code': 'nunique'  # Number of unique goods codes\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "basket_features.columns = ['basket_ID'] + [f\"{col[0]}_{col[1]}\" for col in basket_features.columns[1:]]\n",
    "\n",
    "# Add additional features\n",
    "basket_features['avg_price_per_item'] = basket_features['cash_price_sum'] / basket_features['Nbr_of_prod_purchas_sum']\n",
    "basket_features['price_std_norm'] = basket_features['cash_price_std'] / (basket_features['cash_price_mean'] + 1e-8)\n",
    "\n",
    "print(\"Basket features shape:\", basket_features.shape)\n",
    "print(\"Basket features columns:\", basket_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7177675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with fraud labels\n",
    "df = basket_features.merge(dataset.baskets, left_on='basket_ID', right_on='ID', how='inner')\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "print(\"Merged dataset columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features for fraud detection\n",
    "# These are domain-specific features that might be indicative of fraud\n",
    "\n",
    "# 1. Price anomaly features\n",
    "df['price_anomaly'] = (df['cash_price_sum'] > df['cash_price_sum'].quantile(0.95)).astype(int)\n",
    "df['low_price_anomaly'] = (df['cash_price_sum'] < df['cash_price_sum'].quantile(0.05)).astype(int)\n",
    "\n",
    "# 2. Quantity anomaly features\n",
    "df['quantity_anomaly'] = (df['Nbr_of_prod_purchas_sum'] > df['Nbr_of_prod_purchas_sum'].quantile(0.95)).astype(int)\n",
    "\n",
    "# 3. Diversity features\n",
    "df['item_diversity'] = df['item_nunique'] / (df['Nbr_of_prod_purchas_sum'] + 1e-8)\n",
    "df['make_diversity'] = df['make_nunique'] / (df['Nbr_of_prod_purchas_sum'] + 1e-8)\n",
    "\n",
    "# 4. Price consistency features\n",
    "df['price_consistency'] = 1 / (df['cash_price_std'] + 1e-8)  # Higher values = more consistent prices\n",
    "\n",
    "print(\"Enhanced dataset shape:\", df.shape)\n",
    "print(\"New features added:\", ['price_anomaly', 'low_price_anomaly', 'quantity_anomaly', \n",
    "                              'item_diversity', 'make_diversity', 'price_consistency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf59c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop(['basket_ID', 'ID', 'fraud_flag'], axis=1)\n",
    "y = df['fraud_flag']\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target distribution:\", y.value_counts())\n",
    "print(\"Features:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training fraud rate:\", y_train.mean())\n",
    "print(\"Test fraud rate:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7659e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values before preprocessing\n",
    "print(\"Missing values in features:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "X_train = X_train.fillna(0)  # Fill NaN with 0 for numerical features\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "print(\"Missing values after filling:\")\n",
    "print(X_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe061682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline using skrub's TableVectorizer\n",
    "preprocessor = TableVectorizer(\n",
    "    drop_if_constant=True,\n",
    "    high_cardinality=skrub.TextEncoder(),\n",
    "    low_cardinality=skrub.StringEncoder()\n",
    ")\n",
    "\n",
    "# Test the preprocessor\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(\"Processed training set shape:\", X_train_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d868fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and compare performance\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'auc_score': auc_score\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model (Random Forest typically performs well on tabular data)\n",
    "best_model_name = 'Random Forest'\n",
    "best_pipeline = results[best_model_name]['pipeline']\n",
    "y_pred = results[best_model_name]['predictions']\n",
    "y_pred_proba = results[best_model_name]['probabilities']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"AUC Score: {results[best_model_name]['auc_score']:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Fraud Detection Model Evaluation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {results[best_model_name][\"auc_score\"]:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature Importance (for Random Forest)\n",
    "if hasattr(best_pipeline.named_steps['classifier'], 'feature_importances_'):\n",
    "    feature_names = best_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    importances = best_pipeline.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Get top 10 most important features\n",
    "    top_indices = np.argsort(importances)[-10:]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importances = importances[top_indices]\n",
    "    \n",
    "    axes[1, 0].barh(range(len(top_features)), top_importances)\n",
    "    axes[1, 0].set_yticks(range(len(top_features)))\n",
    "    axes[1, 0].set_yticklabels(top_features)\n",
    "    axes[1, 0].set_xlabel('Feature Importance')\n",
    "    axes[1, 0].set_title('Top 10 Most Important Features')\n",
    "\n",
    "# 4. Model Comparison\n",
    "model_names = list(results.keys())\n",
    "auc_scores = [results[name]['auc_score'] for name in model_names]\n",
    "axes[1, 1].bar(model_names, auc_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "axes[1, 1].set_ylabel('AUC Score')\n",
    "axes[1, 1].set_title('Model Comparison')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(auc_scores):\n",
    "    axes[1, 1].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation to get more robust performance estimates\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(\n",
    "    best_pipeline, X_train, y_train, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation AUC scores: {cv_scores}\")\n",
    "print(f\"Mean CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4795953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation on test set\n",
    "print(\"\\nFinal Model Performance Summary:\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Test AUC: {results[best_model_name]['auc_score']:.4f}\")\n",
    "print(f\"Mean CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef01af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for future use\n",
    "import joblib\n",
    "\n",
    "# Save the complete pipeline\n",
    "joblib.dump(best_pipeline, 'fraud_detection_model.pkl')\n",
    "print(\"Model saved as 'fraud_detection_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc04388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the model for new predictions\n",
    "print(\"\\nExample: Making predictions on new data\")\n",
    "print(\"To use this model on new data, you would:\")\n",
    "print(\"1. Load the model: model = joblib.load('fraud_detection_model.pkl')\")\n",
    "print(\"2. Prepare your data in the same format as X_train\")\n",
    "print(\"3. Make predictions: predictions = model.predict(new_data)\")\n",
    "print(\"4. Get probabilities: probabilities = model.predict_proba(new_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653f870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
