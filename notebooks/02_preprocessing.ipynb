{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90cae02",
   "metadata": {},
   "source": [
    "\n",
    "# Additional preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skrub\n",
    "\n",
    "dataset = skrub.datasets.fetch_employee_salaries()\n",
    "report = skrub.TableReport(dataset.employee_salaries)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51789393",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-learn already provides a range of transformers that can be used for\n",
    "preprocessing data ahead of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618501c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "print(\"Transformer used for preprocesing in scikit-learn\\n\")\n",
    "for name in dir(preprocessing):\n",
    "    if isinstance(obj := getattr(preprocessing, name), type) and issubclass(\n",
    "        obj, BaseEstimator\n",
    "    ):\n",
    "        print(f\"\\t{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3078c9",
   "metadata": {},
   "source": [
    "\n",
    "However, it does not cover some of the use cases that are not necessarily in the\n",
    "scope of scikit-learn.\n",
    "\n",
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Int\": [2, 3, 2],  # Multiple unique values\n",
    "        \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n",
    "        \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n",
    "        \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n",
    "        \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n",
    "        \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565b1a6",
   "metadata": {},
   "source": [
    "\n",
    "What an LLM would do to prepare such dataframe for machine learning?\n",
    "\n",
    "**ðŸ¤– Prompt:**\n",
    "\n",
    "*Given the following dataframe, can you write some pandas code to clean the\n",
    "dataframe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ed9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove columns with all missing values (NaN)\n",
    "df_cleaned = df.dropna(axis=1, how=\"all\")\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove columns with all empty strings\n",
    "df_cleaned = df_cleaned.loc[:, ~(df_cleaned == \"\").all()]\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove constant columns (columns with only one unique value)\n",
    "constant_columns = []\n",
    "for col in df_cleaned.columns:\n",
    "    if df_cleaned[col].nunique() <= 1:\n",
    "        constant_columns.append(col)\n",
    "\n",
    "df_cleaned = df_cleaned.drop(columns=constant_columns)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle remaining missing values\n",
    "\n",
    "# For numerical columns, fill with median\n",
    "numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        median_val = df_cleaned[col].median()\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(median_val)\n",
    "\n",
    "# For categorical columns, fill with mode or 'Unknown'\n",
    "categorical_cols = df_cleaned.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        mode_val = df_cleaned[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(mode_val[0])\n",
    "        else:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(\"Unknown\")\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Clean string columns (remove extra whitespace, handle empty strings)\n",
    "for col in categorical_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        # Strip whitespace\n",
    "        df_cleaned[col] = df_cleaned[col].astype(str).str.strip()\n",
    "        # Replace empty strings with NaN, then fill with 'Unknown'\n",
    "        df_cleaned[col] = df_cleaned[col].replace(\"\", np.nan)\n",
    "        if df_cleaned[col].isnull().any():\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(\"Unknown\")\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert date column to proper datetime format\n",
    "if \"Date\" in df_cleaned.columns:\n",
    "    try:\n",
    "        df_cleaned[\"Date\"] = pd.to_datetime(df_cleaned[\"Date\"], format=\"%d %b %Y\")\n",
    "        print(\"Converted Date column to datetime format\")\n",
    "    except:\n",
    "        print(\"Could not convert Date column to datetime format\")\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = skrub.Cleaner(drop_if_constant=True, n_jobs=-1)\n",
    "cleaner.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472b7f6",
   "metadata": {},
   "source": [
    "\n",
    "**Aside note**: Polars dataframe also works out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "cleaner.fit_transform(pl.from_dataframe(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed3b04",
   "metadata": {},
   "source": [
    "\n",
    "## Date and time feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": [\"2023-01-01 12:34:56\", \"2023-02-15 08:45:23\", \"2023-03-20 18:12:45\"],\n",
    "        \"value\": [10, 20, 30],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cea4fe",
   "metadata": {},
   "source": [
    "\n",
    "What strategy an LLM would do to encode such date and time features?\n",
    "\n",
    "**ðŸ¤– Prompt:**\n",
    "\n",
    "*I'm doing some machine learning using the following data containing date. Could you\n",
    "make some processing such that I can use a linear model later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da46d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime format\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"date\"] = pd.to_datetime(df_encoded[\"date\"])\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa647ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic temporal features\n",
    "df_encoded[\"year\"] = df_encoded[\"date\"].dt.year\n",
    "df_encoded[\"month\"] = df_encoded[\"date\"].dt.month\n",
    "df_encoded[\"day\"] = df_encoded[\"date\"].dt.day\n",
    "df_encoded[\"hour\"] = df_encoded[\"date\"].dt.hour\n",
    "df_encoded[\"minute\"] = df_encoded[\"date\"].dt.minute\n",
    "df_encoded[\"second\"] = df_encoded[\"date\"].dt.second\n",
    "\n",
    "# Extract additional time-based features\n",
    "df_encoded[\"day_of_week\"] = df_encoded[\"date\"].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_encoded[\"day_of_year\"] = df_encoded[\"date\"].dt.dayofyear\n",
    "df_encoded[\"week_of_year\"] = df_encoded[\"date\"].dt.isocalendar().week\n",
    "df_encoded[\"quarter\"] = df_encoded[\"date\"].dt.quarter\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cyclical features for time components\n",
    "# These are important for linear models as they capture the cyclical nature of time\n",
    "\n",
    "# Cyclical encoding for month (1-12)\n",
    "df_encoded[\"month_sin\"] = np.sin(2 * np.pi * df_encoded[\"month\"] / 12)\n",
    "df_encoded[\"month_cos\"] = np.cos(2 * np.pi * df_encoded[\"month\"] / 12)\n",
    "\n",
    "# Cyclical encoding for day of week (0-6)\n",
    "df_encoded[\"day_of_week_sin\"] = np.sin(2 * np.pi * df_encoded[\"day_of_week\"] / 7)\n",
    "df_encoded[\"day_of_week_cos\"] = np.cos(2 * np.pi * df_encoded[\"day_of_week\"] / 7)\n",
    "\n",
    "# Cyclical encoding for hour (0-23)\n",
    "df_encoded[\"hour_sin\"] = np.sin(2 * np.pi * df_encoded[\"hour\"] / 24)\n",
    "df_encoded[\"hour_cos\"] = np.cos(2 * np.pi * df_encoded[\"hour\"] / 24)\n",
    "\n",
    "# Cyclical encoding for day of year (1-365/366)\n",
    "df_encoded[\"day_of_year_sin\"] = np.sin(2 * np.pi * df_encoded[\"day_of_year\"] / 365.25)\n",
    "df_encoded[\"day_of_year_cos\"] = np.cos(2 * np.pi * df_encoded[\"day_of_year\"] / 365.25)\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based categorical features\n",
    "df_encoded[\"is_weekend\"] = (df_encoded[\"day_of_week\"] >= 5).astype(int)\n",
    "df_encoded[\"is_weekday\"] = (df_encoded[\"day_of_week\"] < 5).astype(int)\n",
    "\n",
    "# Time of day categories\n",
    "df_encoded[\"time_of_day\"] = pd.cut(\n",
    "    df_encoded[\"hour\"],\n",
    "    bins=[0, 6, 12, 18, 24],\n",
    "    labels=[\"night\", \"morning\", \"afternoon\", \"evening\"],\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Season based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"autumn\"\n",
    "\n",
    "\n",
    "df_encoded[\"season\"] = df_encoded[\"month\"].apply(get_season)\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05997389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "encoder = make_pipeline(\n",
    "    skrub.ToDatetime(), skrub.DatetimeEncoder(periodic_encoding=\"circular\")\n",
    ")\n",
    "encoder.fit_transform(df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad75f9",
   "metadata": {},
   "source": [
    "\n",
    "## Encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.employee_salaries[\n",
    "    [\n",
    "        \"gender\",\n",
    "        \"department\",\n",
    "        \"department_name\",\n",
    "        \"division\",\n",
    "        \"assignment_category\",\n",
    "        \"employee_position_title\",\n",
    "    ]\n",
    "]\n",
    "skrub.TableReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973daa9",
   "metadata": {},
   "source": [
    "\n",
    "Let's ask an LLM to check what would make sense to encode categorical features.\n",
    "\n",
    "**ðŸ¤– Prompt:**\n",
    "\n",
    "*Given such categories, what strategy would you use to encode low cardinality\n",
    "feature such \"gender\" and high cardinality feature such as \"division\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69650b",
   "metadata": {},
   "source": [
    "\n",
    "**Summary of encoding strategies**\n",
    "\n",
    "For LOW CARDINALITY features (â‰¤ 10 unique values):\n",
    "- **One-Hot Encoding:** Best for nominal categories without inherent order\n",
    "- **Label Encoding:** Simple but can introduce artificial ordering\n",
    "\n",
    "For HIGH CARDINALITY features (> 10 unique values):\n",
    "- **Target Encoding:** Excellent when you have a target variable, handles overfitting\n",
    "- **Frequency Encoding:** Simple, preserves information about category frequency\n",
    "- **Ordinal Encoding:** Good when categories have meaningful order\n",
    "- **GapEncoder:** Advanced method that learns dense representations\n",
    "- **MinHashEncoder:** Good for very high cardinality, approximate but fast\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. `gender` (2 values): One-Hot Encoding\n",
    "2. `assignment_category` (2 values): One-Hot Encoding\n",
    "3. `department` (37 values): Target Encoding or Frequency Encoding\n",
    "4. `division` (694 values): Target Encoding or GapEncoder\n",
    "5. `employee_position_title` (443 values): Target Encoding or GapEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = skrub.StringEncoder()\n",
    "encoder.fit_transform(df[\"division\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23238f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = skrub.TextEncoder(device=\"mps\")\n",
    "encoder.fit_transform(df[\"division\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
